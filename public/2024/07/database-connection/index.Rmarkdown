---
title: 'Strength in Data: Connecting to the Taskmaster Database'
author: Christopher Nam
date: '2024-07-10'
keywords: ["intro", "setup"]
slug: database-connection
section: 
    - intro
    - setup
    - data
series: "Strength in Data"
tags: ["Strength in Data", "Beginner"]
draft: no
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: "default"
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "../",
                      tidy = TRUE
                      )
options(width = 1000)
```

# Your Task

> Successfully connect to the Taskmaster database from within `R`. Fastest wins; your time starts now!

This article provides an overview of *Trabajo de las Mesas*, a pivotal database that will be central to this project.

The article will also provide guidance on how to connect to the database from within `R`.

# *Trabajo de las Mesas* Database

[*Trabajo de las Mesas*](https://tdlm.fly.dev/) (TdlM [^1]) provides a plethora of data associated with Taskmaster in a database format. Data included in the database includes information pertaining to a series, episode, conntestant, task attempts, and even profanity uttered by a contestant.

[^1]: Taskmaster fanatics will know that this is in reference to the hint in S2E5's task *Build a bridge for the potato.*, which has since become one of key pieces of advice for all Taskmaster contestants. It has been suitably adapted for working on data tables in a database, rather than a piece of furniture.

The exhaustive nature of the data truly opens the door to potential questions we may want to answer in the Taskmaster universe. For this reasons, I am immensely grateful to the contributors of this project.

For some musings on TdlM, data quality and assumptions made, see this [post](/themedianduck/2024/07/data-quality-musings/).

# Connecting to the Database from `R`

## Downloading the `.db` file

It is possible to view and query these the numerous tables in TdlM from the [website itself](https://tdlm.fly.dev/). However, this does not lead  intuitively to repeatable and reproduceable analysis. Connecting to the database from a (statistical)        programming language such as `R` or `python`, naturally leads to repeatablility and reproduceability.

I am opting choosing to choose `R` for this project due to my familarity with it, and the high level visualisations and modelling that can be employed.

The tables displayed on the website are powered from the following [database file](https://tdlm.fly.dev/taskmaster.db) which can downloaded and stored locally. The following code chunk downloads the database file locally (based on the repo directory); a corresponding folder location will be created if it does not already exist.

```{r download, message = FALSE}
library(here) #library to help with identifying the repo working directory

# URL where Database file resides. We will download from here.
db_url <- "https://tdlm.fly.dev/taskmaster.db"

# Where the data will be stored locally
db_file_name <- "taskmaster.db"
data_dir <- here("static", "data")

db_data_location <- file.path(data_dir, db_file_name)


# Create Data Directory if does not exist
if(!file.exists(file.path(data_dir))){
    dir.create(file.path(data_dir))
}

# Download file specified by URL, save in the local destination.
if(!file.exists(db_data_location)){
    download.file(url = db_url, destfile = db_data_location, mode = "wb")
}

```

## Connecting to the `.db` file

Now that the database file has been downloaded successfully, we can start to connect to it from `R` directory. The `DBI` package will be employed to establish this connection.

```{r db_connect}
package_name <- "RSQLite"

# Install packages if does not exist, then load.
if(!require(package_name, character.only = TRUE)){
    install.packages(package_name, character.only = TRUE)
} else{
    library(package_name, character.only = TRUE)    
}


# Driver used to establish database connection
sqlite_driver <- dbDriver("SQLite")

# Making the connection 
tm_db <- dbConnect(sqlite_driver, dbname = db_data_location)

```

If successful, we should be able to list all the tables included in the database.

```{r list_tables}
# List all tables that are available in the database
dbListTables(tm_db)
```

## Querying the Database
With the database connection established, we are able to write queries and execute them directly from `R` to access the data. For example:

### A Basic `SELECT` query

```{r cols.print=25, series_output}

# A Basic Select query  on the series table.
query <- "SELECT * FROM series LIMIT 10"

dbGetQuery(tm_db, query)
```

### Advanced query

A more involved query involving `JOIN` and date manipulation is also possible.

```{r max.print=25, advanced_query}
# A join, and data manipulation
query <- "SELECT ts.name,
ts.special as special_flag,
tp.name as champion_name,
tp.seat as chamption_seat,
DATE(ts.studio_end) as studio_end, 
DATE(ts.air_start) as air_start, 
-- Days between air start date, and last studio record date
JULIANDAY(ts.air_start) - JULIANDAY(ts.studio_end) as broadcast_lag_days
FROM series ts -- Series information
LEFT JOIN people tp -- People/Contestant information
    ON ts.id = tp.series
    AND ts.champion = tp.id
WHERE ts.special <> 1 -- Consider regular series
"

results <- dbGetQuery(tm_db, query)

results
```

```{r longest_lag, include = FALSE}
longest_lag_df <- results[which.max(results$broadcast_lag_days),]
```

# A recording to airing insight...
The results of this query already indicate interesting insights; `r longest_lag_df$name` has the largest known delay between studio recording and airing of `r longest_lag_df$broadcast_lag_days` days (approximately `r round(longest_lag_df$broadcast_lag_days/7)` weeks). This is a noticeable deviation from prior series. Future series also seem delayed, although to a lesser extent. 

**Potential followup questions:**
- Could the 2020 pandemic have initiated this lag? 
- Were there other production changes that led to this lag?

# Times Up!
And that concludes this task! Hopefully you've been able to connect to the TdlM database directly through `R` and potentially inspired to start performing your own analysis.
